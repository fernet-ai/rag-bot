{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "p7QF_S96nR2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install unstructured\n",
        "!pip install tiktoken\n",
        "\n",
        "!pip install langchain-community\n",
        "!pip install chromadb\n",
        "!pip install sentence-transformers\n",
        "\n",
        "!pip install scikit-learn\n",
        "!pip install plotly"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lXJXyAaLn1c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0BQW6qakMmj"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/Seminario RAG/data\"\n",
        "\n",
        "def load_documents():\n",
        "  loader = DirectoryLoader(DATA_PATH, glob=\"**/*.md\", recursive=True)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "\n",
        "documents = load_documents()\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400, # 300 caratteri è la dimensione massima che può assumere un chunk\n",
        "        chunk_overlap=100, # È il numero di caratteri sovrapposti tra un chunk e il successivo -> Serve per mantenere il contesto tra i blocchi, specialmente se il testo viene tagliato a metà di una frase o concetto.\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "\n",
        "# source: da quale document è preso questo chunk; index: da dove inizia questo chunk\n",
        "document = chunks[2]\n",
        "\n",
        "print(document.page_content)\n",
        "print(document.metadata)\n"
      ],
      "metadata": {
        "id": "xauJlBMWnw1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.vectorstores import Chroma\n",
        "# from langchain_community.embeddings import OpenAIEmbeddings\n",
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"xxxxx\"\n",
        "\n",
        "# CHROMA_PATH = \"/content/drive/MyDrive/Colab Notebooks/Seminario RAG/chroma\"\n",
        "\n",
        "# if os.path.exists(CHROMA_PATH):\n",
        "#   shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "# # Creazione del DB vettoriale a partire dai chunks\n",
        "# db = Chroma.from_documents(\n",
        "#     chunks, embedding=OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
        "#     )\n",
        "# db.persist()\n",
        "\n"
      ],
      "metadata": {
        "id": "ACRJpVCsriWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "# A differenza di OpenAI, i modelli Hugging Face non richiedono chiavi API e non consumano crediti.\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # Piccolo, veloce e gratuito\n",
        ")\n",
        "\n",
        "\n",
        "#CHROMA_PATH = \"/content/drive/MyDrive/Colab Notebooks/Seminario RAG/chroma\"\n",
        "CHROMA_PATH = \"/content/chroma_temp\"\n",
        "\n",
        "\n",
        "if os.path.exists(CHROMA_PATH):\n",
        "  shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "# Creazione del DB vettoriale a partire dai chunks\n",
        "db = Chroma.from_documents(\n",
        "    chunks, embedding=embedding_model, persist_directory=CHROMA_PATH\n",
        ")\n",
        "db.persist()\n",
        "\n"
      ],
      "metadata": {
        "id": "VfZaru6WzRHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea la funzione di embedding\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Ottiene il vettore embedding per la parola \"apple\"\n",
        "vector = embedding_function.embed_query(\"apple\")\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "USSTHXY-3oyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "embed1 = embedding_function.embed_query(\"radio\")\n",
        "embed2 = embedding_function.embed_query(\"dog\")\n",
        "\n",
        "# Calcola la similarità tra i due embedding\n",
        "similarity = cosine_similarity([embed1], [embed2])[0][0]\n",
        "print(f\"Cosine similarity: {similarity}\")"
      ],
      "metadata": {
        "id": "4arr3DtL4ToF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### VISUALIZZARE EMBEDDINGS IN UNO SPAZIO A 3 DIMENSIONI\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 1. Frasi da visualizzare\n",
        "texts = [\"apple\", \"orange\", \"banana\", \"fruit\", \"laptop\", \"computer\", \"AI\", \"machine learning\"]\n",
        "\n",
        "# 2. Calcola gli embedding\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embeddings = [embedding_model.embed_query(text) for text in texts]\n",
        "\n",
        "# 3. Riduzione a 3 dimensioni con PCA\n",
        "pca = PCA(n_components=3)\n",
        "embeddings_3d = pca.fit_transform(embeddings)\n",
        "\n",
        "# 4. Visualizzazione con matplotlib 3D\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "for i, text in enumerate(texts):\n",
        "    x, y, z = embeddings_3d[i]\n",
        "    ax.scatter(x, y, z)\n",
        "    ax.text(x, y, z, text)\n",
        "\n",
        "ax.set_title(\"Visualizzazione 3D degli Embedding\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8wI3Gj62-yNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Frasi da visualizzare\n",
        "texts = [\"apple\", \"orange\", \"banana\", \"fruit\", \"laptop\", \"computer\", \"AI\", \"machine learning\"]\n",
        "\n",
        "# Calcola gli embedding\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embeddings = [embedding_model.embed_query(text) for text in texts]\n",
        "\n",
        "# Riduci a 3 dimensioni con PCA\n",
        "pca = PCA(n_components=3)\n",
        "embeddings_3d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Visualizzazione interattiva 3D con Plotly\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=embeddings_3d[:, 0],\n",
        "    y=embeddings_3d[:, 1],\n",
        "    z=embeddings_3d[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=texts,\n",
        "    textposition=\"top center\",\n",
        "    marker=dict(size=8, opacity=0.8),\n",
        ")])\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Embedding 3D Visualization',\n",
        "    scene=dict(\n",
        "        xaxis_title='PCA 1',\n",
        "        yaxis_title='PCA 2',\n",
        "        zaxis_title='PCA 3'\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "RT7l7mW9_JX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dobbiamo trovare i nostri chunks nel db vettoriale che più probabilmente rispondono alla domanda che diamo in input\n",
        "\n",
        "query_text = \"Cosa dedica la rivista Life ad Andy Warhol?\"\n",
        "\n",
        "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
        "\n",
        "if not results or results[0][1] < 0.2: # soglia di accettazione\n",
        "    print(\"Nessun risultato rilevante trovato.\") # Il risultato può essere influito dalla dimensione dei chunk (magari hanno contesto troppo piccolo)\n",
        "else:\n",
        "  for doc, score in results:\n",
        "      print(\"Score:\", score)\n",
        "      print(\"Document content:\\n\", doc.page_content)\n",
        "      print(\"---\")\n"
      ],
      "metadata": {
        "id": "9Y9tdbKk7J7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trovati i chunk rilevanti, possiamo dobbiamo creare una risposta rilevante a partire da questi chunk\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "query_text = \"Cosa dedica la rivista Life ad Andy Warhol?\"\n",
        "\n",
        "\n",
        "# Crea un PROMPT TEMPLATE ( ti crei un bel prompt naive inserendoci il chunk (o i chunks) come contesto)\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {query}\n",
        "\"\"\"\n",
        "# Unisci i contenuti testuali dei documenti trovati\n",
        "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
        "\n",
        "# Crea il template e formatta il prompt\n",
        "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "prompt = prompt_template.format(context=context_text, query=query_text)\n",
        "\n",
        "print(prompt) # Prompt finale che andrà in pasto all'LLM"
      ],
      "metadata": {
        "id": "8D-uyWttCrcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dai la risposta finale con LLM\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Con GPT\n",
        "# model = ChatOpenAI()\n",
        "# response_text = model.predict(prompt)\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "prompt = \"Cosa dedica la rivista Life ad Andy Warhol?\"\n",
        "response = generator(prompt, max_length=100, do_sample=True)\n",
        "print(response[0]['generated_text'])\n",
        "\n"
      ],
      "metadata": {
        "id": "APVmK1IQEhAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fornisci un riferimento alle fonti da cui ha tratto la risposta\n",
        "response_text = response[0]['generated_text']\n",
        "\n",
        "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
        "formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
        "print(formatted_response)\n"
      ],
      "metadata": {
        "id": "u555qPnPFmt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}